<section xml:id="s_Rn_dot_product">
    <title>Inner product structure of <m>\R^n</m></title>
    <introduction>
        <p>
            In this section we introduce another operation on <m>\R^n</m> called the <em>dot product</em>. As we will see, the dot product is an additional layer added to the vector space structure of <m>\R^n</m> that gives rise to a number of useful analytic tools. More generally, the dot product turns out to be just one example of what is called an <em>inner product</em> on a vector space. Inner products imbue vector spaces with valuable geometric content, a few of which are highlighted below. 
            <ul>
              <li>
                <title>Distance and angle</title>
                <p>
                  A notion of <em>distance</em> and <em>angle</em> between two vectors can be defined relative to a given inner product. These provide a numeric measurement of how <q>close</q> (distance) or <q>closely oriented</q> (angle) two vectors in our space are.
                </p>
              </li>
              <li>
                <title>Orthogonality</title>
                <p>
                Two vectors of an inner product space are <em>orthogonal</em> if their inner product is equal to zero. Orthogonality leads to a general notion of <em>orthogonal projection</em>, and is part of the definition of an <em>orthogonal basis</em>. These are two computationally very important concepts in linear algebra that we will take up in earnest later.  
                </p>
              </li>
            </ul>
          </p>
    </introduction>
    <subsection xml:id="ss_dot_product">
        <title>Dot product and inner products</title>
        <p>
            We will give a purely arithmetic definition of the dot product on <m>\R^n</m>, and will provide retroactive geometric motivation in <xref ref="ss_length_angles"/>.
        </p>
        <definition xml:id="d_dot_product">
            <title>Dot product</title>
            <statement>
                <p>
                    Given <m>n</m>-vectors <m>\boldx=(x_1,x_2,\dots, x_n)</m> and <m>\boldy=(y_1,y_2,\dots, y_n)</m> we define their <term>dot product</term> <m>\boldx\cdot\boldy</m> as 
                    <me>
                        \boldx\cdot\boldy=\sum_{i=1}^nx_iy_i=x_1y_1+x_2y_2+\cdots +x_ny_n
                    </me>.
                    The operation 
                    <md>
                        <mrow>\R^n\times \R^n \amp\rightarrow \R^n </mrow>
                        <mrow>(\boldx,\boldy) \amp \mapsto \boldx\cdot \boldy</mrow>
                    </md>
                    is called the <term>dot product</term> on <m>\R^n</m>. 
                </p>
                <p>
                    The vector space <m>\R^n</m> together with the dot product is called <term>Euclidean <m>n</m>-space </term>
                </p>
            </statement>
        </definition>
        <example xml:id="eg_dot_product">
            <title>Dot product on <m>\R^4</m></title>
            <statement>
              <p>
              Let <m>\boldx=(-1,2,0,1), \boldy=(1,2,1,1)</m>. Then
              <me>
                \boldx\cdot \boldy=-1+4+0+1=4
              </me>,
              and
              <me>
                \boldx\cdot\boldx=1+4+0+1=6
              </me>.
              </p>
            </statement>
          </example>
        <p>
           As mentioned above, the dot product is just one example of a more general notion called an inner product, which is an additional operation defined on a vector space.  
        </p>
        <definition xml:id="d_innerproduct">
            <title>Inner product</title>
            <statement>
              <p>
                Let <m>V</m> be a vector space.
                An <term>inner product</term> on <m>V</m> is an operation that takes as input a pair of vectors <m>\boldv, \boldw\in V</m> and outputs a scalar <m>\langle \boldv, \boldw \rangle \in \R</m>. Using function notation:
                <md>
                <mrow>\langle \ , \rangle \colon \amp V\times V\rightarrow \R</mrow>
                <mrow>(\boldv_1,\boldv_2)\amp \mapsto \langle \boldv_1,\boldv_2\rangle</mrow>
                </md>.
                Furthermore, this operation must satisfy the following axioms.
                <ol marker="i">
                  <li>
                    <title>Symmetry</title>
                    <p>
                    For all <m>\boldv, \boldw\in V</m> we have
                    <me>
                      \langle \boldv, \boldw \rangle =\langle \boldw, \boldv \rangle
                    </me>.
                    </p>
                  </li>
                  <li>
                    <title>Linearity</title>
                    <p>
                      For all <m>\boldv, \boldw, \boldu\in V</m> and <m>c, d\in \R</m> we have :
                      <me>
                        \langle c\boldv+d\boldw, \boldu \rangle =c \langle \boldv, \boldu \rangle +d \langle \boldw, \boldu \rangle
                      </me>.
                      It follows by (i) (symmetry) that
                      <me>
                        \langle \boldu, c\boldv+d\boldw \rangle =c \langle \boldu, \boldv \rangle +d \langle \boldu, \boldw \rangle
                      </me>.
                    </p>
                  </li>
                  <li>
                    <title>Positive definiteness</title>
                    <p>
                      For all <m>\boldv\in V</m> we have
                      <md>
                        <mrow> \langle \boldv, \boldv \rangle \amp\geq 0,\text{ and} \amp (\text{positivity})</mrow>
                        <mrow> \langle \boldv, \boldv \rangle \amp=0 \text{ if and only if } \boldv=\boldzero \amp (\text{definiteness}) </mrow>
                      </md>.
                    </p>
                  </li>
                </ol>
                An <term>inner product space</term> is a pair <m>(V, \langle , \rangle )</m>, where <m>V</m> is a vector space, and <m>\langle , \rangle </m> is a choice of inner product on <m>V</m>.
              </p>
            </statement>
          </definition>
        
            <remark xml:id="rm_innerproduct_algebra">
            <title>Inner products of linear combinations</title>
            <statement>
              <p>
              We will have many opportunities to <q>expand out</q> an inner product of two linear combinations of vectors. Using axioms (i) and (ii) in series, this process resembles the procedure for multiplying two polynomials. For example, we have
              <md>
                <mrow>\langle c\boldv+d\boldw, e\boldv+f\boldw\rangle
                \amp = c \langle \boldv,  e\boldv+f\boldw \rangle +d \langle \boldw, e\boldv+f\boldw \rangle \amp (<xref ref="d_innerproduct" text="global"/>, \text{(ii)})</mrow>
                <mrow> \amp=ce \langle \boldv, \boldv\rangle+cf\langle \boldv,\boldw \rangle +de \langle \boldw, \boldv\rangle+df\langle \boldw, \boldw\rangle \amp (<xref ref="d_innerproduct" text="global"/>, \text{(ii)}) </mrow>
                <mrow>  \amp = ce\langle \boldv, \boldv\rangle +(cf+de)\langle \boldv, \boldw\rangle +df\langle \boldw, \boldw\rangle
                \amp (<xref ref="d_innerproduct" text="global"/>, \text{(i)})</mrow>
              </md>.
              Note how in the last step we are able to group the <q>cross terms</q>, <m>\langle \boldv, \boldw\rangle=\langle \boldw, \boldv\rangle</m> using the symmetry axiom.
            </p>
              <p>
                More generally, given linear combinations
              <md>
                <mrow>
                \boldv \amp = c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n=\sum_{i=1}^n c_i\boldv_i </mrow>
                <mrow> \boldw \amp =d_1\boldv_1+d_2\boldv_2+\cdots +d_n\boldv_n=\sum_{i=1}^nd_i\boldv_i </mrow>
              </md>,
              the same reasoning shows that
              <md>
                <mrow>\langle \boldv, \boldw \rangle   \amp=
                c_1d_1 \langle \boldv_1, \boldv_1  \rangle+c_2d_2 \langle \boldv_2, \boldv_2 \rangle +\cdots+c_nd_n \langle \boldv_n, \boldv_n \rangle +\underset{\text{cross terms}}{\underbrace{(c_1d_2+c_2d_1)\langle \boldv_1,\boldv_2  \rangle +\cdots}}  </mrow>
                <mrow> \amp= \sum_{i=1}^nc_id_i \langle \boldv_i, \boldv_i \rangle +\underset{\text{cross terms}}{\underbrace{\sum_{1\leq i\lt j\leq n}(c_{i}d_j+c_jd_i)\langle \boldv_i, \boldv_j \rangle}}  </mrow>
              </md>.
              In particular, we have
              <me>
                \langle \boldv, \boldv\rangle =\sum_{i=1}^nc_i^2 \langle \boldv_i, \boldv_i \rangle +\sum_{1\leq i\lt j\leq n}2c_ic_j \langle \boldv_i, \boldv_j \rangle
              </me>.
              </p>
            </statement>
          </remark>
          <p>
           Although we will almost exclusively work with the dot product in this treatment of linear algebra, it is worth considering a natural family of inner products on <m>\R^n</m> that the dot product fits nicely into: namely, <em>weighted dot products</em>. These examples of inner products are especially important in data science. 
          </p>
          <definition xml:id="d_weighted_dot_product">
            <title>Weighted dot product</title>
            <statement>
                <p>
                    Let <m>k_1, k_2, \dots , k_n</m> be a list of positive real numbers: <ie/>, <m>k_i&gt; 0</m> for all <m>1\leq i\leq n</m>. The <term>weighted dot product</term> with weights <m>(k_1,k_2,\dots, k_n)</m> is the operation 
                    <md>
                        <mrow>\R^n\times\R^n \amp \rightarrow \R</mrow>
                        <mrow>(\boldx,\boldy) \amp \mapsto \angvec{\boldx,\boldy}</mrow>
                    </md>
                    defined as on <m>n</m>-vectors <m>\boldx=(x_1,x_2,\dots, x_n)</m> and <m>\boldy=(y_1,y_2,\dots, y_n)</m> as follows: 
                    <men xml:id="eq_weighted_dot_product">
                        \angvec{\boldx,\boldy}=\sum_{i=1}^nk_ix_iy_i=k_1x_1y_1+k_2x_2y_2+\cdots k_nx_ny_n
                    </men>.
                    When <m>k_i=1</m> for all <m>1\leq i\leq n</m>, the weighted dot product is equal to the dot product. 
                </p>
            </statement>
          </definition>
          <example xml:id="eg_weigh_dot">
            <title>Weighted dot product</title>
              <statement>
                <p>
                The dot product with weights <m>(2, 1, 3)</m> on <m>\R^3</m> is defined as
                <me>
                  \langle \boldx, \boldy  \rangle= 2x_1y_1+x_2y_2+3x_3y_3
                </me>.
                Let <m>\boldx=(-1,-1,-1)</m> and <m>\boldy=(1,0,1)</m>. We have
                <me>
                  \langle \boldx, \boldy \rangle =2(-1)+0-3=-5
                </me>,
                and
                <me>
                  \langle \boldx, \boldx \rangle =2(-1)^2+1(-1)^2+3(-1)^2=2+1+3=6
                </me>.
                </p>
              </statement>
            </example>
          <theorem xml:id="th_weighted_dot_product">
            <title>Weighted dot product</title>
            <statement>
                <p>
                    Let <m>k_1, k_2, \dots , k_n</m> be a list of positive real numbers. The weighted dot product on <m>\R^n</m> with weights <m>(k_1,k_2,\dots, k_n)</m> is an inner product. In particular, the dot product is an inner product. 
                </p>
            </statement>
            <proof>
                <p>
                   We verify each axiom in turn. Throughout we assume <m>\boldx=(x_1,x_2,\dots, x_n)</m>, <m>\boldy=(y_1,y_2,\dots, y_n)</m>, and <m>\boldw=(w_1,w_2,\dots, w_n)</m> are arbitrary elements of <m>\R^n</m>. 
                </p>
                <case>
                <title>Axiom i</title>
                <p>
                  We have 
                  <md>
                    <mrow>\angvec{\boldx,\boldy} \amp = \sum_{k=1}^n k_ix_iy_i \amp \text{(def.)}</mrow>
                    <mrow> \amp = \sum_{k=1}^n k_iy_ix_i \amp \text{(comm. prop. of real mult.)}</mrow>
                    <mrow> \amp = \angvec{\boldy,\boldx} \amp \text{(by def.)}</mrow> 
                  </md>.
                </p>
                </case>
                <case>
                <title>Axiom ii</title>
                <p>
                Given scalars <m>c,d\in \R</m>, we have 
                <me>
                  c\boldx+d\boldy=(cx_1+dy_1,cx_2+dy_2,\dots, cx_n+dy_n)
                </me>,
                and thus
                <md>
                  <mrow>\angvec{c\boldx+d\boldy, \boldw} \amp = \sum_{i=1}^n k_i(cx_i+dy_i)w_i \amp \text{(def.)}</mrow>
                  <mrow> \amp = \sum_{i=1}^nck_ix_iw_i+dk_iy_iw_i \amp \text{(dist. prop. of reals)}  </mrow>
                  <mrow> \amp = c\sum_{i=1}^nk_ix_iw_i+d\sum_{i=1}^nk_iy_iw_i \amp \text{(props. of real arith.)} </mrow>
                  <mrow> \amp = c\angvec{\boldx,\boldw}+d\angvec{\boldy,boldw}</mrow>
                </md>.
                </p>
                </case>
                <case>
                <title>Axiom iii</title>
                <p>
                We have 
                <me>
                  \langle \boldx, \boldx \rangle=k_1x_1^2+k_2x_2^2+\cdots k_nx_n^2
                </me>.
                Since <m>k_i&gt; 0</m> and <m>x_i^2\geq 0</m> for all <m>i</m> (squares of real numbers are nonnegative), we have <m>\langle \boldx, \boldx \rangle\geq 0</m> for any <m>\boldx</m>.
                </p>
                <p>
                  We now show that <m>\angvec{\boldx,\boldx}=0</m> if and only if <m>\boldx=\boldzero=(0,0,\dots, 0)</m>. The reverse implication is clear: if <m>x_i=0</m> for all <m>i</m>, then <m>\sum_{i=1}^nk_ix_i^2=\sum_{i=1}^n 0=0</m>. We prove the forward implication by showing that its contrapositive is true: <ie/>, if <m>\boldx\ne \boldzero</m>, then <m>\angvec{\boldx,\boldx} \ne 0</m>. If <m>\boldx\ne \boldzero</m>, then we have <m>x_{i_0}\ne 0</m> for some <m>1\leq i_0\leq n</m>, in which case <m>k_{i_0}x_{i_0}^2 &gt; 0</m>. But then 
                  <md>
                    <mrow> \angvec{\boldx,\boldx} \amp = k_{i_0}x_{i_0}^2+\sum_{i\ne i_0}k_ix_i^2 </mrow>
                    <mrow> \amp \geq k_{i_0}x_{i_0}^2 \amp (k_ix_i^2\geq 0)</mrow>
                    <mrow> \amp &gt; 0 </mrow>
                  </md>.
                  In particular, <m>\angvec{\boldx,\boldx}\ne 0</m>, as desired. 
                </p>
                </case>
            </proof>
          </theorem>
          <example xml:base="eg_weights_positive">
            <title>Why the weights must be positive</title>
            <statement>
              <p>
              Consider the operation on <m>\R^2</m> defined as
              <me>
                \langle \boldx, \boldy \rangle =(-1)x_1y_1+2x_2y_2
              </me>
              where <m>\boldx=(x_1,x_2), \boldy=(y_1,y_2)</m>. This operation satisfies axioms i-ii of <xref ref="d_innerproduct"/>. (See proof of <xref ref="th_weighted_dot_product"/>.) However, it fails both the positivity and definiteness properties of axiom (iii): <eg/>, 
              <md>
                <mrow>\langle (3,1), (3,1) \rangle \amp =-9+2=-7\lt 0</mrow>
                <mrow> \langle (1,1/\sqrt{2}), (1,1/\sqrt{2})\rangle \amp=-1+2/2=0 </mrow>
              </md>.
              </p>
            </statement>
          </example>
    </subsection>
    
     
      
    <subsection xml:id="ss_length_angles">
        <title>Length and angles in inner product spaces</title>
        <p>
            As mentioned above, once an inner product is established, we can define further notions like length, distance, and angle in terms of the given inner product.  When the inner product in question is the standard dot product on <m>\R^2</m> or <m>\R^3</m>, then these are precisely the familiar notions you may have met in multivariable calculus. 
          </p>
          <definition xml:id="d_norm">
            <title>Length (or norm) of a vector</title>
            <idx><h>length (or norm)</h><h>of a vector</h></idx>
            <notation>
              <usage><m>\norm{\boldv}</m></usage>
              <description>length (or norm) of <m>\boldv</m></description>
            </notation>
            <statement>
              <p>
                Let <m>(V, \langle ,  \rangle )</m> be an inner product space. Given <m>\boldv\in V</m>, its <term>length</term> (or <term>norm</term>), denoted <m>\norm{\boldv}, is defined </m> as
                <me>
                  \norm{\boldv}=\sqrt{\langle \boldv, \boldv  \rangle }
                </me>.
                A <term>unit vector</term> is a vector <m>\boldv</m> of length one: <ie />, a vector  <m>\boldv</m> satisfying <m>\norm{\boldv}=1</m>.
              </p>
            </statement>
          </definition>
          <example xml:id="eg_norm_dot">
            <title>Norm with respect to dot product</title>
            <statement>
              <p>
                Consider <m>V=\R^4</m> with the standard dot product. Compute <m>\norm{(1,-1,-2,1)}</m>.
              </p>
            </statement>
            <solution>
              <p>
                We have
                <md>
                  <mrow> \norm{(1,-1,-2,1)}\amp =\sqrt{(1,-1,-2,1)\cdot (1,-1,-2,1)} </mrow>
                  <mrow> \amp=\sqrt{1+1+4+1} </mrow>
                  <mrow>  \amp = \sqrt{7}</mrow>
                </md>.
              </p>
            </solution>
          </example>
          <example xml:id="eg_norm_weighteddot">
              <title>Norm with respect to weighted dot product</title>
              <statement>
              <p>
                Consider <m>V=\R^3</m> equipped with the dot product with weights <m>(1,2,3)</m>. Compute <m>\norm{(3,1,-2)}</m>.
              </p>
            </statement>
            <solution>
              <p>
                We have
                <md>
                  <mrow> \norm{(3,1,-2)}\amp =\sqrt{\left\langle (3,1,-2), (3,1,-2)\right\rangle} </mrow>
                  <mrow> \amp=\sqrt{1(3^2)+2(1^2)+3((-2)^2)} </mrow>
                  <mrow>  \amp = \sqrt{23}</mrow>
                </md>.
              </p>
            </solution>
          </example>
          <remark xml:id="rm_unit_vectors">
            <title>Unit vectors</title>
      
            <statement>
              <p>
                Given any <m>\boldv\ne \boldzero\in V</m>, the vector <m>\boldu=\frac{1}{\norm{\boldv}}\boldv</m> is a unit vector. To verify this, let <m>c=\norm{\boldv}</m> and compute
                <md>
                  <mrow>\norm{\boldu} \amp =\sqrt{\left\langle \frac{1}{c}\boldv,\frac{1}{c}\boldv   \right\rangle }</mrow>
                  <mrow> \amp =\sqrt{\frac{1}{c^2}\langle \boldv,\boldv  \rangle} \amp (<xref ref="d_innerproduct"/>, \text{(ii)})  </mrow>
                  <mrow>  \amp =\frac{1}{\val{c}}\sqrt{\langle \boldv,\boldv  \rangle } </mrow>
                  <mrow>  \amp =\frac{1}{c}\sqrt{\langle \boldv,\boldv  \rangle } \amp (c=\norm{\boldv}\geq 0)</mrow>
                  <mrow>  \amp =\frac{\norm{\boldv}}{\norm{\boldv}}=1</mrow>
                </md>.
              </p>
            </statement>
          </remark>
          <example xml:id="eg_unit_vectors">
            <title>Unit vectors</title>
            <statement>
              <p>
                For each inner product space <m>(V, \langle\,, \rangle)</m> and <m>\boldv\in V</m> compute the associated unit vector <m>\boldu=\frac{1}{\norm{v}}\boldv</m>
                <ol>
                  <li>
                    <p>
                      <m>V=\R^4</m> with dot product, <m>\boldv=(1,-1,2,1)</m>
                    </p>
                  </li>
                  <li>
                    <p>
                      <m>V=\R^3</m> with dot product with weights <m>(1,2,3)</m>, <m>\boldv=(3,1,-2)</m>
                    </p>
                  </li>
                </ol>
              </p>
            </statement>
            <solution>
              <p>
                The norms of the vectors in each case were computed in <xref first="eg_norm_dot" last="eg_norm_weighteddot"/>. We simply scale to compute the corresponding unit vectors.
              </p>
              <ol>
                <li>
                  <p>
                    <m>\boldu=\frac{1}{\sqrt{7}}(1,-1,2,1)=(\sqrt{7}/7,-\sqrt{7}/7,2\sqrt{7}/7,\sqrt{7}/7)</m>
                  </p>
                </li>
                <li>
                  <p>
                    <m>\boldu=\frac{1}{\sqrt{23}}(3,1,-2)=(3\sqrt{23}/23,\sqrt{23}/23,-2\sqrt{23}/23)</m>
                  </p>
                </li>
              </ol>
            </solution>
          </example>
          <p>
          Next, we define the distance between two vectors in an inner product space as the length of their vector difference.
          </p>
          <definition xml:id="d_distance">
            <idx><h>distance</h><h>between two vectors</h></idx>
            <notation>
              <usage><m>d(\boldv, \boldw)</m></usage>
              <description>the distance between <m>\boldv</m> and <m>\boldw</m></description>
            </notation>
            <title>Distance between vectors</title>
            <statement>
              <p>
              Let <m>(V,\langle ,  \rangle )</m> be an inner product space. The <term>distance between <m>\boldv, \boldw\in V</m></term>, denoted <m>d(\boldv, \boldw)</m>, is defined as
              <me>
                d(\boldv, \boldw)=\norm{\boldv-\boldw}=\sqrt{\langle \boldv-\boldw,\boldv-\boldw  \rangle }
              </me>.
              </p>
            </statement>
          </definition>
          <example xml:id="eg_distance">
            <statement>
              <p>
               For each inner product space <m>V</m>, compute the distance between the given vectors.
              </p>
              <ol>
                <li>
                  <p>
                    <m>V=\R^3</m> with the dot product, <m>\boldx=(x_1,x_2,x_3)</m>, <m>\boldy=(y_1,y_2,y_3)</m>
                  </p>
                </li>
                <li>
                  <p>
                    <m>V=\R^3</m> with the dot product with weights <m>(1,2,3)</m>, <m>\boldx=(x_1,x_2,x_3)</m>, <m>\boldy=(y_1,y_2,y_3)</m>
                  </p>
                </li>
              </ol>
            </statement>
            <solution>
              <ol>
                <li>
                  <p>
                    We have
                    <md>
                      <mrow>d(\boldx,\boldy) \amp = \norm{\boldx-\boldy} </mrow>
                      <mrow>  \amp = \sqrt{\langle \boldx-\boldy,\boldx-\boldy\rangle} </mrow>
                      <mrow> \amp =\sqrt{(x_1-y_1,x_2-y_2,x_3-y_3)\cdot(x_1-y_1,x_2-y_2,x_3-y_3)} </mrow>
                      <mrow>  \amp = \sqrt{(x_1-y_1)^2+(x_2-y_2)^2+(x_3-y_3)^2}</mrow>
                    </md>.
                  </p>
                </li>
                <li>
                  <p>
                    We have
                    <md>
                      <mrow>d(\boldx,\boldy) \amp = \norm{\boldx-\boldy} </mrow>
                      <mrow>  \amp = \sqrt{\langle \boldx-\boldy,\boldx-\boldy\rangle} </mrow>
                      <mrow> \amp =\sqrt{\angvec{(x_1-y_1,x_2-y_2,x_3-y_3),(x_1-y_1,x_2-y_2,x_3-y_3)}} </mrow>
                      <mrow>  \amp = \sqrt{1(x_1-y_1)^2+2(x_2-y_2)^2+3(x_3-y_3)^2}</mrow>
                    </md>.
                  </p>
                </li>
                
              </ol>
            </solution>
          </example>
          <theorem xml:id="th_norm_basic_props">
            <title>Basic properties of length and distance</title>
            <statement>
              <p>
              Let <m>(V,\langle ,  \rangle)</m> be an inner product space.
              </p>
              <ol>
                <li>
                  <p>
                    For all <m>\boldv\in V</m> we have
                    <me>
                      \norm{\boldv}\geq 0
                    </me>,
                    and equality holds if and only if <m>\boldv=0</m>.
                  </p>
                </li>
                <li>
                  <p>
                    For all <m>c\in \R</m> and <m>\boldv\in V</m> we have
                    <me>
                      \norm{c\boldv}=\val{c}\norm{\boldv}
                    </me>.
                  </p>
                </li>
                <li>
                  <p>
                    For all <m>\boldv, \boldw\in V</m> we have
                    <me>
                      d(\boldv,\boldw)=d(\boldw,\boldv)\geq 0
                    </me>,
                    and equality holds if and only if <m>\boldv=\boldw</m>.
                  </p>
                </li>
              </ol>
            </statement>
            <proof>
              <p>
                We prove (2) and leave the rest as an exercise (<xref ref="ex_norm_props"/>).
              </p>
              <p>
                Given <m>c\in \R</m> and <m>\boldv\in V</m> we have
                <md>
                  <mrow>\norm{c\boldv} \amp =\sqrt{\langle c\boldv, c\boldv\rangle}</mrow>
                  <mrow> \amp=\sqrt{c^2\langle \boldv, \boldv\rangle} \amp (<xref ref="d_innerproduct"/>)</mrow>
                  <mrow>  \amp =\val{c}\sqrt{\langle \boldv, \boldv\rangle} \amp (\sqrt{c^2}=\val{c})</mrow>
                </md>.
              </p>
            </proof>
      
          </theorem>
    </subsection>
    <subsection>
        <title>Cauchy-Schwarz inequality, triangle inequalities, and angles between vectors</title>
        <p>
          The famous <em>Cauchy-Schwarz inequality</em> has a knack of cropping up all over the world of science: from properties of covariance in statistics, to the Heisenberg uncertainty principle of quantum mechanics. More directly pertinent to our discussion, the Cauchy-Schwarz inequality implies the triangle inequalities (<xref ref="th_triangle_inequalities" text="global"/>) and ensures that our notion of the angle between two nonzero vectors (<xref ref="d_angle"/>) is well-defined.
        </p>
        <theorem xml:id="th_Cauchy-Schwarz">
          <title>Cauchy-Schwarz inequality</title>
          <statement>
            <p>
              Let <m>(V,\langle \ , \rangle)</m> be an inner product space.
              For all <m>\boldv,\boldw\in V</m> we have
              <me>
              \val{\langle\boldv,\boldw\rangle}\leq\norm{\boldv}\norm{\boldw}
              </me>,
              and equality holds if and only if <m>\boldv=c\boldw</m> for some <m>c\in\R</m>.
            </p>
          </statement>
        </theorem>
        <proof>
          <p>
            Fix vectors <m>\boldv</m> and <m>\boldw</m>.
            For any <m>t\in\R</m> we have by positivity
            <me>
            0\leq \langle t\boldv-\boldw,t\boldv-\boldw\rangle=\langle\boldv,\boldv\rangle t^2-2\langle\boldv,\boldw\rangle t+\langle\boldw,\boldw\rangle=at^2-2bt+c
            </me>,
            where
            <men xml:id="eq_cauchy_schwarz">
             a=\langle\boldv,\boldv\rangle, \ b=\langle\boldv,\boldw\rangle, \   c=\langle\boldw,\boldw\rangle=\norm{w}^2
           </men>.
    
            Since <m>at^2-2b\,t+c\geq 0</m> for all <m>t\in \R</m> the quadratic polynomial <m>p(t)=at^2-2b\,t+c</m> has
            <em>at most</em> one root. Using the quadratic formula we conclude that we must have <m>4b^2-4ac\leq 0</m>,
            since otherwise <m>p(t)</m> would have two distinct roots.
            It follows that
            <me>
              4\langle \boldv, \boldw\rangle^2-4\norm{\boldv}^2\norm{\boldw}^2\leq 0
            </me>,
            or equivalently
            <me>
            \langle\boldv,\boldw\rangle^2\leq \norm{\boldv}^2\norm{\boldw}^2
            </me>.
            Taking square-roots yields the desired inequality.
          </p>
          <p>
            The same reasoning shows that the Cauchy-Schwarz inequality is an actual equality if and only if  <m>p(t)=0</m> for some <m>t</m> if and only if
            <m>0=\langle t\boldv-\boldw,t\boldv-\boldw\rangle</m> if and only if  <m>\boldv=t\boldw</m> for some <m>t</m>
            (by positivity).
          </p>
        </proof>
        <p>
          The following triangle inequalities are more or less direct consequences of the Cauchy-Schwarz inequality.
        </p>
        <theorem xml:id="th_triangle_inequalities">
          <title>Triangle Inequalities</title>
          <statement>
            <p>
              Let <m>(V, \langle \ , \rangle)</m> be an inner product space.
              <ol>
                <li>
                  <p>
                    For all <m>\boldv, \boldw\in V</m> we have <me>\norm{\boldv+\boldw}\leq \norm{\boldv}+\norm{\boldw}</me>.
                  </p>
                </li>
                <li>
                  <p>
                    For all <m>\boldv, \boldw, \boldu\in V</m> we have
                    <me>d(\boldv,\boldw)\leq d(\boldv,\boldu)+d(\boldu,\boldw)</me>
                  </p>
                </li>
              </ol>
            </p>
          </statement>
          <proof>
            <p>
              This is an elementary exercise of unpacking the definitions of norm and distance in terms of the inner product, and then applying the Cauchy-Schwarz inequality appropriately. The proof is left as an exercise.
            </p>
          </proof>
        </theorem>
        <p>
          Let <m>(V, \langle ,  \rangle )</m> be an inner product space. For any <em>nonzero</em> vectors <m>\boldv, \boldw</m>, the Cauchy-Schwarz inequality tells us that
          <me>
            \val{\langle \boldv, \boldw \rangle }\leq \norm{\boldv}\, \norm{\boldw}
          </me>,
          or equivalently,
          <me>
            -1\leq \frac{\langle \boldv, \boldw \rangle}{\norm{\boldv}\, \norm{\boldw}} \leq 1
          </me>.
          It follows that there is a unique real number <m>\theta\in [0,\pi]</m> satisfying
          <me>
            \cos\theta=\frac{\langle \boldv, \boldw \rangle}{\norm{\boldv}\, \norm{\boldw}}
          </me>.
          We call <m>\theta</m> the <em>angle between <m>\boldv</m> and <m>\boldw</m></em>.
        </p>
        <definition xml:id="d_angle">
          <title>Angle between vectors</title>
          <idx><h>angle</h><h>between vectors</h></idx>
          <statement>
            <p>
              Let <m>(V,\langle ,  \rangle )</m> be an inner product space. Given nonzero vectors <m>\boldv, \boldw\in V</m>, the <term>angle between <m>\boldv</m> and <m>\boldw</m></term> is defined to be the unique <m>\theta\in [0,\pi]</m> satisfying
              <me>
                \cos\theta=\frac{\langle \boldv, \boldw \rangle}{\norm{\boldv}\, \norm{\boldw}}
              </me>.
              Equivalently, we have
              <me>
                \theta=\arccos\left(
                  \frac{\langle \boldv, \boldw \rangle}{\norm{\boldv}\, \norm{\boldw}}
                \right)
              </me>.
            </p>
          </statement>
        </definition>
            <remark xml:id="rm_general_angles">
            <title>Definition of angle via inner product</title>
            <statement>
            <p>
              Our definition of the angle between two vectors may remind you of the dot product angle formula for vectors in <m>\R^3</m>:
              <men xml:id="eq_dotproductangle">
                \cos\theta=\frac{\boldx\cdot\boldy}{\norm{\boldx}\norm{\boldy}}
              </men>.
              Interestingly, whereas <xref ref="eq_dotproductangle"/> is typically treated as a <em>theorem</em>, derived from properties of the dot product and the law of cosines, in a general inner product space the equation
              <me>
                \cos\theta =\frac{\langle \boldv, \boldw\rangle}{\norm{\boldv}\norm{\boldw}}
              </me>
              is understood as the <em>definition</em> of the angle between two vectors.
            </p>
          </statement>
        </remark>
        <example xml:id="eg_angle_dotproduct">
          <statement>
            <p>
              Consider <m>\R^2</m> along with the dot product. Verify that our definition of the angle <m>\theta</m> between <m>(1,1)</m> and <m>(1,0)</m> is consistent with our planar geometry notion of angle.
            </p>
          </statement>
          <solution>
            <p>
              According to <xref ref="d_angle"/>, <m>\theta</m> is the unique element of <m>[0,\pi]</m> satisfying
              <me>
                \cos \theta=\frac{(1,1)\cdot (1,0)}{\norm{(1,1)}\norm{(1,0)}}=\frac{1}{\sqrt{2}}=\frac{\sqrt{2}}{2}
              </me>.
              We recognize <m>\theta</m> as the familiar angle <m>\pi/4</m>, as expected.
    
            </p>
          </solution>
        </example>
        <example xml:id="eg_angle_weighted">
          <statement>
            <p>
              Consider <m>\R^2</m> with the weighted dot product
              <me>
                \langle (x_1,x_2), (y_1,y_2)\rangle=2x_1x_2+y_1y_2
              </me>
              Compute the angle <m>\theta</m> between <m>(1,1)</m> and <m>(0,0)</m> with respect to this inner product
            </p>
          </statement>
          <solution>
            <p>
              First compute
              <md>
                <mrow>\langle (1,1), (1,0)\rangle \amp= 2(1)+1(0)=2 </mrow>
                <mrow> \norm{(1,1)}\amp =\sqrt{\langle (1,1),(1,1)\rangle} </mrow>
                <mrow>  \amp =\sqrt{2+1}=\sqrt{3} </mrow>
                <mrow> \norm{(1,0)}\amp =\sqrt{\langle (1,0),(1,0)\rangle} </mrow>
                <mrow>  \amp =\sqrt{2} </mrow>
              </md>
              By definition <m>\theta</m> is the unique value in <m>[0,\pi]</m> satisfying
              <me>
                \cos\theta=\frac{\langle (1,1), (1,0)\rangle}{\norm{(1,1)}\norm{(1,0)}}=\frac{2}{\sqrt{3}\sqrt{2}}=\frac{\sqrt{6}}{3}
              </me>.
              We see that <m>\theta</m> is not one of our familiar angles from the unit circle (<eg />, <m>\pi/6, \pi/4</m>, <etc />) and so express <m>\theta</m> in terms of the <m>\arccos</m> function:
              <me>
                \theta=\arccos(\sqrt{6}/3)\approx 35.3^\circ
              </me>.
            </p>
          </solution>
        </example>
      </subsection>
      <subsection>
        <title>Choosing your inner product</title>
        <p>
          Why, given a fixed vector space <m>V</m>,
          would we prefer one inner product definition to another?
          One way of understanding a particular choice of inner product is to ask what its corresponding notion of distance measures.
        </p>
        <example xml:id="eg_why_weightedproduct">
          <title>Weighted dot product distance</title>
          <statement>
            <p>
             Consider <m>\R^n</m> with a choice of weighted dot product
             <me>
               \langle (x_1,x_2,\dots, x_n), (y_1,y_2,\dots, y_n)\rangle=k_1x_1y_1+k_2x_2y_2+\cdots +k_nx_ny_n,
             </me>
             where <m>k_1,k_2,\dots, k_n</m> are fixed positive constants. With respect to this inner product the distance between two vectors <m>\boldx=(x_1,x_2,\dots, x_n)</m> and <m>\boldy=(y_1,y_2,\dots, y_n)</m> is
             <me>
               d(\boldx,\boldy)=\norm{\boldx-\boldy}=\sqrt{k_1(x_1-y_1)^2+k_2(x_2-y_2)^2+\cdots +k_n(x_n-y_n)^2}
             </me>.
             Thus <m>d(\boldx, \boldy)</m> is an aggregate measure of the difference between the corresponding entries of <m>\boldx</m> and <m>\boldy</m>, as weighted by our choice of the constants <m>k_i</m>.
           </p>
           <p>
             Imagine that each element of <m>\boldx\in \R^n</m> is a data point collected by measuring <m>n</m> different properties of a sample <m>s</m> : <ie />, <m>x_i</m> is the measured value of property <m>P_i</m> on <m>s</m> for all <m>1\leq i\leq n</m>.  Given samples <m>s</m> and <m>s'</m> with corresponding measurement vectors <m>\boldx</m> and <m>\boldy</m>, the weighted distance <m>d(\boldx,\boldy)</m> is then a quantitative way of saying how <q>close</q> the two samples are to one another. The choice of weights <m>k_i</m> allows us to adjust the relative influence of a given property <m>P_i</m> in determining this closeness. For example, the standard dot product (<m>k_i=1</m> for all <m>i</m>) yields a notion of distance that gives each property equal standing.
            </p>
          </statement>
        </example>
      </subsection>
      <!-- <xi:include href="./s_innerproducts_ex.ptx"/> -->
</section>